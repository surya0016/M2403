Practical No. 1: Bitwise Operations
def bitwise_operation(a, b):
    # AND operation
    res = a & b
    print(f"AND operation between {a} and {b}: {res}")
    
    # OR operation
    res1 = a | b
    print(f"OR operation between {a} and {b}: {res1}")
    
    # XOR operation
    res2 = a ^ b
    print(f"XOR operation between {a} and {b}: {res2}")
    
    # NOT operation
    res3 = ~a
    print(f"NOT operation between {a}: {res3}")
    res6 = ~b
    print(f"NOT operation between {b}: {res6}")
    
    # Left shift
    res4 = a << 1
    print(f"LEFT shift operation between {a} and {b}: {res4}")
    
    # Right shift
    res5 = a >> 1
    print(f"RIGHT shift operation between {a} and {b}: {res5}")

n1 = int(input("Enter a binary number: "))
n2 = int(input("Enter a binary number: "))
bitwise_operation(n1, n2)

-------------------------------------------------------------------
Practical No. 1: Text File Processing
import networkx as nx
import matplotlib.pyplot as plt

# Create a directed graph
G = nx.DiGraph()

# Add edges to the graph
G.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4), (4, 5)])

# Calculate PageRank
pagerank = nx.pagerank(G)

# Print PageRank values
print("PageRank values:", pagerank)

# Visualize the graph
nx.draw(G, with_labels=True, node_color='lightblue', edge_color='gray')
plt.show()

-------------------------------------------------------------------
Practical No. 2: PageRank Algorithm
import networkx as nx
import matplotlib.pyplot as plt

# Create a directed graph
G = nx.DiGraph()

# Add edges to the graph
G.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4), (4, 5)])

# Calculate PageRank
pagerank = nx.pagerank(G)

# Print PageRank values
print("PageRank values:", pagerank)

# Visualize the graph
nx.draw(G, with_labels=True, node_color='lightblue', edge_color='gray')
plt.show()

-------------------------------------------------------------------
Practical No. 3: Inverted Index
from collections import defaultdict
import re

# Sample documents
documents = {
    1: "The quick brown fox jumps over the lazy dog",
    2: "The brown fox is fast and the dog is lazy",
    3: "London bridge is falling down"
}

# Function to tokenize and preprocess the document
def preprocess(text):
    text = text.lower()
    tokens = re.findall(r'\w+', text)
    return set(tokens)

# Create inverted index
inverted_index = defaultdict(set)

for doc_id, doc_text in documents.items():
    terms = preprocess(doc_text)
    for term in terms:
        inverted_index[term].add(doc_id)

# Query function
def query(query_text):
    terms = preprocess(query_text)
    results = set(doc_id for term in terms for doc_id in inverted_index.get(term, []))
    return results

# Example query
query_result = query("brown fox")
print("Documents containing 'brown' and 'fox':", query_result)

-------------------------------------------------------------------
Practical No. 4: Levenshtein Distance
def leven(x, y):
    n = len(x)
    m = len(y)
    A = [[i + j for j in range(m + 1)] for i in range(n + 1)]
    
    for i in range(n):
        for j in range(m):
            A[i + 1][j + 1] = min(
                A[i][j + 1] + 1,
                A[i + 1][j] + 1,
                A[i][j] + int(x[i] != y[j])
    
    return A[n][m]

print(leven("brap", "rap"))
print(leven("trial", "try"))
print(leven("hello", "heo"))
print(leven("kit", "kite"))

-------------------------------------------------------------------
Practical No. 5: HITS Algorithm
import networkx as nx

# Create a directed graph
G = nx.DiGraph()

# Add edges to the graph
G.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4), (4, 5)])

# Calculate authority and hub scores
authority_scores, hub_scores = nx.hits(G)

print("Authority Scores:", authority_scores)
print("Hub Scores:", hub_scores)

-------------------------------------------------------------------
Practical No. 6: Jaccard, Cosine, Manhattan, Euclidean Distance
import numpy as np
from collections import Counter

# Jaccard Similarity function
def jaccard_similarity(doc1, doc2):
    words_doc1 = set(doc1.split())
    words_doc2 = set(doc2.split())
    intersection = words_doc1.intersection(words_doc2)
    union = words_doc1.union(words_doc2)
    return len(intersection) / len(union)

# Cosine Similarity function
def cosine_similarity(doc1, doc2):
    words = list(set(doc1.split()).union(set(doc2.split())))
    vec1 = [doc1.split().count(word) for word in words]
    vec2 = [doc2.split().count(word) for word in words]
    dot_product = np.dot(vec1, vec2)
    magnitude1 = np.linalg.norm(vec1)
    magnitude2 = np.linalg.norm(vec2)
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    return dot_product / (magnitude1 * magnitude2)

# Manhattan Distance function
def manhattan_distance(doc1, doc2):
    words = list(set(doc1.split()).union(set(doc2.split())))
    vec1 = [doc1.split().count(word) for word in words]
    vec2 = [doc2.split().count(word) for word in words]
    return sum(abs(a - b) for a, b in zip(vec1, vec2))

# Euclidean Distance function
def euclidean_distance(doc1, doc2):
    words = list(set(doc1.split()).union(set(doc2.split())))
    vec1 = [doc1.split().count(word) for word in words]
    vec2 = [doc2.split().count(word) for word in words]
    return np.sqrt(sum((a - b) ** 2 for a, b in zip(vec1, vec2)))

# Example documents
doc1 = "I like to play football"
doc2 = "I like playing football"

# Calculate metrics
jaccard = jaccard_similarity(doc1, doc2)
cosine = cosine_similarity(doc1, doc2)
manhattan = manhattan_distance(doc1, doc2)
euclidean = euclidean_distance(doc1, doc2)

# Print results
print(f"Jaccard Similarity: {jaccard}")
print(f"Cosine Similarity: {cosine}")
print(f"Manhattan Distance: {manhattan}")
print(f"Euclidean Distance: {euclidean}")

-------------------------------------------------------------------
Practical No. 7: Permuted Index & K-Gram
from collections import defaultdict

# Function to generate permuted index
def generate_permuted_index(documents):
    permuted_index = defaultdict(list)
    
    for doc_id, doc_text in documents.items():
        words = doc_text.split()
        for i, word in enumerate(words):
            # Create a permuted term by rotating the phrase
            permuted_term = ' '.join(words[i:] + words[:i])
            permuted_index[permuted_term].append((doc_id, i))
    
    return permuted_index

# Function to generate k-grams
def generate_kgrams(word, k=2):
    kgrams = []
    for i in range(len(word) - k + 1):
        kgrams.append(word[i:i+k])
    return kgrams

# Sample documents
documents = {
    1: "the quick brown fox",
    2: "jumps over the lazy dog",
    3: "the quick fox jumps over"
}

# Generate permuted index
permuted_index = generate_permuted_index(documents)
print("Permuted Index:")
for term, doc_ids in permuted_index.items():
    print(f"{term}: {doc_ids}")

# Generate k-grams for a word
word = "quick"
kgrams = generate_kgrams(word, k=2)
print(f"\nK-Grams for '{word}': {kgrams}")

-------------------------------------------------------------------
Practical No. 8: Stopwords and Preprocessing
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function to preprocess text
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation and special characters
    text = re.sub(r'[^\w\s]', '', text)
    
    # Tokenize the text
    tokens = word_tokenize(text)
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    
    return filtered_tokens

# Example text
text = "The quick brown fox jumps over the lazy dog. The dog was not amused!"

# Preprocess the text
processed_tokens = preprocess_text(text)
print("Processed Tokens:", processed_tokens)

-------------------------------------------------------------------
Practical No. 9: Collaborative Filtering
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Sample data
data = {
    'user': ['Alice', 'Bob', 'Alice', 'Alice', 'Charlie'],
    'item': ['Item1', 'Item1', 'Item2', 'Item3', 'Item1'],
    'rating': [5, 4, 3, 4, 5]
}

df = pd.DataFrame(data)

# Create user-item matrix
user_item_matrix = df.pivot_table(index='user', columns='item', values='rating', fill_value=0)
print(user_item_matrix)

# Calculate cosine similarity
similarity = cosine_similarity(user_item_matrix)
print(similarity)

-------------------------------------------------------------------
Practical No. 10: Web Crawlers
import requests
from bs4 import BeautifulSoup

def crawl(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    return [link.get('href') for link in soup.find_all('a', href=True) if link.get('href').startswith('http')]

links = crawl('https://siesascs.edu.in')  # Replace with the URL you want to crawl
print(links)

-------------------------------------------------------------------
Practical No. 11: Extractive Summary
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import nltk

nltk.download("stopwords")
nltk.download('punkt')

def extractive_summary(text, num_sentence=3):
    sentences = sent_tokenize(text)
    stop_words = set(stopwords.words('english'))
    cleaned_sentences = [''.join([word for word in nltk.word_tokenize(s.lower()) if word not in stop_words and word.isalnum()]) for s in sentences]
    tfidf_matrix = TfidfVectorizer().fit_transform(cleaned_sentences)
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    scores = {i: sum(cosine_sim[i]) for i in range(len(sentences))}
    return ' '.join([sentences[i] for i in sorted(scores, key=scores.get, reverse=True)[:num_sentence]])

text = "Atoms of radioactive elements can split. According to Albert Einstein, mass and energy are interchangeable under certain circumstances. When atoms split, the process is called nuclear fission. In this case, a small amount of mass is converted into energy. Thus the energy released cannot do much damage. However, several subatomic particles called neutrons are also emitted during this process. Each neutron will hit a radioactive element releasing more neutrons in the process. This causes a chain reaction and creates a large amount of energy. This energy is converted into heat which expands uncontrollably causing an explosion. Hence, atoms do not literally explode. They generate energy that can cause explosions."

summary = extractive_summary(text)
print(summary)
